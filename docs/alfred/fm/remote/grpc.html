<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>alfred.fm.remote.grpc API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>alfred.fm.remote.grpc</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import ast
import json
import logging
import socket
import sys
from concurrent import futures
from typing import Optional, Union, Iterable, Tuple

import grpc

from alfred.fm.query import Query, RankedQuery, CompletionQuery
from alfred.fm.remote.protos import query_pb2
from alfred.fm.remote.protos import query_pb2_grpc
from alfred.fm.remote.utils import get_ip
from alfred.fm.response import RankedResponse, CompletionResponse

logger = logging.getLogger(__name__)


class gRPCClient:

    def __init__(self,
                 host: str,
                 port: int,
                 credentials: Optional[Union[grpc.ChannelCredentials, str]] = None,
                 ):
        self.host = host
        self.port = port

        if credentials:
            self.channel = grpc.secure_channel(
                f&#34;{self.host}:{self.port}&#34;, credentials)
        else:
            self.channel = grpc.insecure_channel(f&#34;{self.host}:{self.port}&#34;)

        try:
            grpc.channel_ready_future(self.channel).result(timeout=10)
        except grpc.FutureTimeoutError:
            sys.exit(&#39;Error connecting to server&#39;)
        else:
            self.stub = query_pb2_grpc.QueryServiceStub(self.channel)

    @staticmethod
    def _interpret_msg(msg):
        candidate = None
        if isinstance(msg, CompletionQuery):
            msg = msg.load()[0]
        elif isinstance(msg, RankedQuery):
            msg, candidate = msg.prompt, msg.get_answer_choices_str()
        elif isinstance(msg, Tuple):
            msg, candidate = msg[0], msg[1]
        return msg, candidate

    def run(self,
            msg: Union[str, Query, Tuple[str, str]],
            **kwargs,
            ):

        kwargs = json.dumps(kwargs)
        msg, candidate = self._interpret_msg(msg)
        response = self.stub.Inference(
            query_pb2.InferenceRequest(
                message=msg,
                candidate=candidate,
                kwargs=kwargs))
        if candidate:
            resp = self.stub.DataReady(
                query_pb2.DataReadySignal(
                    data_size=1, kwargs=kwargs))
            for response in resp:
                # Only supposed to run once
                response = RankedResponse(
                    **{&#34;prediction&#34;: response.message, &#34;scores&#34;: ast.literal_eval(response.logit)})
        else:
            response = CompletionResponse(response.message)
        return response

    def run_dataset(self,
                    dataset: Union[Iterable[Query],
                    Iterable[str],
                    Iterable[Tuple[str,
                    str]]],
                    **kwargs,
                    ):
        try:
            data_size = len(dataset)
        except TypeError:
            data_size = -1

        kwargs = json.dumps(kwargs)

        self.stub.DataHeader(query_pb2.DataHeaderRequest(data_size=data_size))

        candidate = None

        for msg in dataset:
            msg, candidate = self._interpret_msg(msg)
            self.stub.Inference(
                query_pb2.InferenceRequest(
                    message=msg, candidate=candidate))

        responses = []
        for response in self.stub.DataReady(
                query_pb2.DataReadySignal(
                    data_size=data_size,
                    kwargs=kwargs)):
            if candidate:
                response = {
                    &#34;prediction&#34;: response.message,
                    &#34;scores&#34;: ast.literal_eval(
                        response.logit)}
                response = RankedResponse(**response)
            else:
                response = CompletionResponse(response.message)
            responses.append(response)

        return responses


class gRPCServer(query_pb2_grpc.QueryServiceServicer):
    &#34;&#34;&#34;
    Manages connections with gRPCClient
    &#34;&#34;&#34;

    @staticmethod
    def port_finder(port: int) -&gt; int:
        &#34;&#34;&#34;
        Finds the next available port if given port is not available
        &#34;&#34;&#34;
        while True:
            try:
                s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                s.bind((&#39;&#39;, port))
                s.close()
                return port
            except OSError:
                port -= 1
                logger.warning(
                    f&#34;Port {port + 1} is not available, trying {port}&#34;)

    def __init__(self,
                 model,
                 port: int = 10719,
                 credentials: Optional[grpc.ServerCredentials] = None,
                 ):
        self.model = model
        self.port = self.port_finder(port)

        self.server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
        query_pb2_grpc.add_QueryServiceServicer_to_server(self, self.server)

        if credentials:
            self.server.add_secure_port(f&#34;[::]:{self.port}&#34;, credentials)
        else:
            self.server.add_insecure_port(f&#34;[::]:{self.port}&#34;)

        self.group_infer_flag = False
        self.group_ranked_flag = False
        self.dataset = []
        self.candidates = []

        my_ip = get_ip()
        hostname = socket.gethostname()
        # Will not work if the port is not visible to the outside world
        logger.info(
            f&#34;gRPC server starting at {hostname}:{self.port} or {my_ip}:{self.port}&#34;)
        self.server.start()
        self.server.wait_for_termination()

    def Inference(self, request, context):
        instance = request.message
        candidate = request.candidate
        kwargs = request.kwargs

        # kwargs is optional
        if kwargs:
            kwargs = json.loads(kwargs)
        else:
            kwargs = {}

        if candidate:
            instance = RankedQuery(
                prompt=instance,
                candidates=candidate.split(&#34;|||&#34;))
            self.group_ranked_flag = True
            self.group_infer_flag = True
        else:
            self.group_ranked_flag = False

        if self.group_infer_flag:
            self.dataset.append(instance)
            return query_pb2.InferenceResponse(message=&#34;&#34;, success=True)
        else:
            try:
                logger.info(
                    f&#34;Received inference request: {instance} with kwargs: {kwargs}&#34;)
                response = self.model.run(instance, **kwargs)
                response = response.prediction
                logger.info(f&#34;Sending inference response: {response}&#34;)
                # TODO: Take advantage of serialization of Responses
                return query_pb2.InferenceResponse(
                    message=response, ranked=False, success=True)
            except Exception as e:
                message = f&#34;Error: {e}&#34;
                logger.error(message)
                return query_pb2.InferenceResponse(
                    message=message, ranked=False, success=False)

    def DataReady(self, request, context):
        dataset_size = request.data_size
        kwargs = request.kwargs

        # kwargs is optional
        if kwargs:
            kwargs = json.loads(kwargs)
        else:
            kwargs = {}
        logger.info(f&#34;Received {dataset_size} queries&#34;)
        if len(self.dataset) != dataset_size:
            logger.warning(
                f&#34;Dataset size mismatch, expected {dataset_size} but got {len(self.dataset)}&#34;)

        logger.info(f&#34;Running {dataset_size} queries&#34;)
        # edge case for 1 rankequery
        for response in self.model.run(self.dataset, **kwargs):
            # TODO: Take advantage of serialization of Responses
            if isinstance(response, RankedResponse):
                yield query_pb2.InferenceResponse(message=response.prediction,
                                                  logit=str(
                                                      response.scores) if response.scores else None,
                                                  ranked=response.scores is not None, success=True)
            elif isinstance(response, CompletionResponse):
                yield query_pb2.InferenceResponse(message=response.prediction, ranked=False, success=True)
            else:
                yield query_pb2.InferenceResponse(message=response, ranked=False, success=True)
        logger.info(f&#34;Finished running {dataset_size} queries&#34;)
        self.group_infer_flag, self.group_ranked_flag = False, False
        self.dataset = []
        logger.info(f&#34;Resetting dataset&#34;)

    def DataHeader(self, request, context):
        self.group_infer_flag = True
        self.dataset = []
        dataset_size = request.data_size
        return query_pb2.DataHeaderResponse(data_size=dataset_size)

    def close(self):
        self.server.stop(0)

    def restart(self):
        self.close()
        self.__init__(self.port)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="alfred.fm.remote.grpc.gRPCClient"><code class="flex name class">
<span>class <span class="ident">gRPCClient</span></span>
<span>(</span><span>host: str, port: int, credentials: Union[grpc.ChannelCredentials, str, None] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class gRPCClient:

    def __init__(self,
                 host: str,
                 port: int,
                 credentials: Optional[Union[grpc.ChannelCredentials, str]] = None,
                 ):
        self.host = host
        self.port = port

        if credentials:
            self.channel = grpc.secure_channel(
                f&#34;{self.host}:{self.port}&#34;, credentials)
        else:
            self.channel = grpc.insecure_channel(f&#34;{self.host}:{self.port}&#34;)

        try:
            grpc.channel_ready_future(self.channel).result(timeout=10)
        except grpc.FutureTimeoutError:
            sys.exit(&#39;Error connecting to server&#39;)
        else:
            self.stub = query_pb2_grpc.QueryServiceStub(self.channel)

    @staticmethod
    def _interpret_msg(msg):
        candidate = None
        if isinstance(msg, CompletionQuery):
            msg = msg.load()[0]
        elif isinstance(msg, RankedQuery):
            msg, candidate = msg.prompt, msg.get_answer_choices_str()
        elif isinstance(msg, Tuple):
            msg, candidate = msg[0], msg[1]
        return msg, candidate

    def run(self,
            msg: Union[str, Query, Tuple[str, str]],
            **kwargs,
            ):

        kwargs = json.dumps(kwargs)
        msg, candidate = self._interpret_msg(msg)
        response = self.stub.Inference(
            query_pb2.InferenceRequest(
                message=msg,
                candidate=candidate,
                kwargs=kwargs))
        if candidate:
            resp = self.stub.DataReady(
                query_pb2.DataReadySignal(
                    data_size=1, kwargs=kwargs))
            for response in resp:
                # Only supposed to run once
                response = RankedResponse(
                    **{&#34;prediction&#34;: response.message, &#34;scores&#34;: ast.literal_eval(response.logit)})
        else:
            response = CompletionResponse(response.message)
        return response

    def run_dataset(self,
                    dataset: Union[Iterable[Query],
                    Iterable[str],
                    Iterable[Tuple[str,
                    str]]],
                    **kwargs,
                    ):
        try:
            data_size = len(dataset)
        except TypeError:
            data_size = -1

        kwargs = json.dumps(kwargs)

        self.stub.DataHeader(query_pb2.DataHeaderRequest(data_size=data_size))

        candidate = None

        for msg in dataset:
            msg, candidate = self._interpret_msg(msg)
            self.stub.Inference(
                query_pb2.InferenceRequest(
                    message=msg, candidate=candidate))

        responses = []
        for response in self.stub.DataReady(
                query_pb2.DataReadySignal(
                    data_size=data_size,
                    kwargs=kwargs)):
            if candidate:
                response = {
                    &#34;prediction&#34;: response.message,
                    &#34;scores&#34;: ast.literal_eval(
                        response.logit)}
                response = RankedResponse(**response)
            else:
                response = CompletionResponse(response.message)
            responses.append(response)

        return responses</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="alfred.fm.remote.grpc.gRPCClient.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, msg: Union[str, <a title="alfred.fm.query.query.Query" href="../query/query.html#alfred.fm.query.query.Query">Query</a>, Tuple[str, str]], **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self,
        msg: Union[str, Query, Tuple[str, str]],
        **kwargs,
        ):

    kwargs = json.dumps(kwargs)
    msg, candidate = self._interpret_msg(msg)
    response = self.stub.Inference(
        query_pb2.InferenceRequest(
            message=msg,
            candidate=candidate,
            kwargs=kwargs))
    if candidate:
        resp = self.stub.DataReady(
            query_pb2.DataReadySignal(
                data_size=1, kwargs=kwargs))
        for response in resp:
            # Only supposed to run once
            response = RankedResponse(
                **{&#34;prediction&#34;: response.message, &#34;scores&#34;: ast.literal_eval(response.logit)})
    else:
        response = CompletionResponse(response.message)
    return response</code></pre>
</details>
</dd>
<dt id="alfred.fm.remote.grpc.gRPCClient.run_dataset"><code class="name flex">
<span>def <span class="ident">run_dataset</span></span>(<span>self, dataset: Union[Iterable[<a title="alfred.fm.query.query.Query" href="../query/query.html#alfred.fm.query.query.Query">Query</a>], Iterable[str], Iterable[Tuple[str, str]]], **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_dataset(self,
                dataset: Union[Iterable[Query],
                Iterable[str],
                Iterable[Tuple[str,
                str]]],
                **kwargs,
                ):
    try:
        data_size = len(dataset)
    except TypeError:
        data_size = -1

    kwargs = json.dumps(kwargs)

    self.stub.DataHeader(query_pb2.DataHeaderRequest(data_size=data_size))

    candidate = None

    for msg in dataset:
        msg, candidate = self._interpret_msg(msg)
        self.stub.Inference(
            query_pb2.InferenceRequest(
                message=msg, candidate=candidate))

    responses = []
    for response in self.stub.DataReady(
            query_pb2.DataReadySignal(
                data_size=data_size,
                kwargs=kwargs)):
        if candidate:
            response = {
                &#34;prediction&#34;: response.message,
                &#34;scores&#34;: ast.literal_eval(
                    response.logit)}
            response = RankedResponse(**response)
        else:
            response = CompletionResponse(response.message)
        responses.append(response)

    return responses</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="alfred.fm.remote.grpc.gRPCServer"><code class="flex name class">
<span>class <span class="ident">gRPCServer</span></span>
<span>(</span><span>model, port: int = 10719, credentials: Optional[grpc.ServerCredentials] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Manages connections with gRPCClient</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class gRPCServer(query_pb2_grpc.QueryServiceServicer):
    &#34;&#34;&#34;
    Manages connections with gRPCClient
    &#34;&#34;&#34;

    @staticmethod
    def port_finder(port: int) -&gt; int:
        &#34;&#34;&#34;
        Finds the next available port if given port is not available
        &#34;&#34;&#34;
        while True:
            try:
                s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                s.bind((&#39;&#39;, port))
                s.close()
                return port
            except OSError:
                port -= 1
                logger.warning(
                    f&#34;Port {port + 1} is not available, trying {port}&#34;)

    def __init__(self,
                 model,
                 port: int = 10719,
                 credentials: Optional[grpc.ServerCredentials] = None,
                 ):
        self.model = model
        self.port = self.port_finder(port)

        self.server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
        query_pb2_grpc.add_QueryServiceServicer_to_server(self, self.server)

        if credentials:
            self.server.add_secure_port(f&#34;[::]:{self.port}&#34;, credentials)
        else:
            self.server.add_insecure_port(f&#34;[::]:{self.port}&#34;)

        self.group_infer_flag = False
        self.group_ranked_flag = False
        self.dataset = []
        self.candidates = []

        my_ip = get_ip()
        hostname = socket.gethostname()
        # Will not work if the port is not visible to the outside world
        logger.info(
            f&#34;gRPC server starting at {hostname}:{self.port} or {my_ip}:{self.port}&#34;)
        self.server.start()
        self.server.wait_for_termination()

    def Inference(self, request, context):
        instance = request.message
        candidate = request.candidate
        kwargs = request.kwargs

        # kwargs is optional
        if kwargs:
            kwargs = json.loads(kwargs)
        else:
            kwargs = {}

        if candidate:
            instance = RankedQuery(
                prompt=instance,
                candidates=candidate.split(&#34;|||&#34;))
            self.group_ranked_flag = True
            self.group_infer_flag = True
        else:
            self.group_ranked_flag = False

        if self.group_infer_flag:
            self.dataset.append(instance)
            return query_pb2.InferenceResponse(message=&#34;&#34;, success=True)
        else:
            try:
                logger.info(
                    f&#34;Received inference request: {instance} with kwargs: {kwargs}&#34;)
                response = self.model.run(instance, **kwargs)
                response = response.prediction
                logger.info(f&#34;Sending inference response: {response}&#34;)
                # TODO: Take advantage of serialization of Responses
                return query_pb2.InferenceResponse(
                    message=response, ranked=False, success=True)
            except Exception as e:
                message = f&#34;Error: {e}&#34;
                logger.error(message)
                return query_pb2.InferenceResponse(
                    message=message, ranked=False, success=False)

    def DataReady(self, request, context):
        dataset_size = request.data_size
        kwargs = request.kwargs

        # kwargs is optional
        if kwargs:
            kwargs = json.loads(kwargs)
        else:
            kwargs = {}
        logger.info(f&#34;Received {dataset_size} queries&#34;)
        if len(self.dataset) != dataset_size:
            logger.warning(
                f&#34;Dataset size mismatch, expected {dataset_size} but got {len(self.dataset)}&#34;)

        logger.info(f&#34;Running {dataset_size} queries&#34;)
        # edge case for 1 rankequery
        for response in self.model.run(self.dataset, **kwargs):
            # TODO: Take advantage of serialization of Responses
            if isinstance(response, RankedResponse):
                yield query_pb2.InferenceResponse(message=response.prediction,
                                                  logit=str(
                                                      response.scores) if response.scores else None,
                                                  ranked=response.scores is not None, success=True)
            elif isinstance(response, CompletionResponse):
                yield query_pb2.InferenceResponse(message=response.prediction, ranked=False, success=True)
            else:
                yield query_pb2.InferenceResponse(message=response, ranked=False, success=True)
        logger.info(f&#34;Finished running {dataset_size} queries&#34;)
        self.group_infer_flag, self.group_ranked_flag = False, False
        self.dataset = []
        logger.info(f&#34;Resetting dataset&#34;)

    def DataHeader(self, request, context):
        self.group_infer_flag = True
        self.dataset = []
        dataset_size = request.data_size
        return query_pb2.DataHeaderResponse(data_size=dataset_size)

    def close(self):
        self.server.stop(0)

    def restart(self):
        self.close()
        self.__init__(self.port)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="alfred.fm.remote.protos.query_pb2_grpc.QueryServiceServicer" href="protos/query_pb2_grpc.html#alfred.fm.remote.protos.query_pb2_grpc.QueryServiceServicer">QueryServiceServicer</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="alfred.fm.remote.grpc.gRPCServer.port_finder"><code class="name flex">
<span>def <span class="ident">port_finder</span></span>(<span>port: int) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the next available port if given port is not available</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def port_finder(port: int) -&gt; int:
    &#34;&#34;&#34;
    Finds the next available port if given port is not available
    &#34;&#34;&#34;
    while True:
        try:
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            s.bind((&#39;&#39;, port))
            s.close()
            return port
        except OSError:
            port -= 1
            logger.warning(
                f&#34;Port {port + 1} is not available, trying {port}&#34;)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="alfred.fm.remote.grpc.gRPCServer.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def close(self):
    self.server.stop(0)</code></pre>
</details>
</dd>
<dt id="alfred.fm.remote.grpc.gRPCServer.restart"><code class="name flex">
<span>def <span class="ident">restart</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def restart(self):
    self.close()
    self.__init__(self.port)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="alfred.fm.remote.protos.query_pb2_grpc.QueryServiceServicer" href="protos/query_pb2_grpc.html#alfred.fm.remote.protos.query_pb2_grpc.QueryServiceServicer">QueryServiceServicer</a></b></code>:
<ul class="hlist">
<li><code><a title="alfred.fm.remote.protos.query_pb2_grpc.QueryServiceServicer.DataHeader" href="protos/query_pb2_grpc.html#alfred.fm.remote.protos.query_pb2_grpc.QueryServiceServicer.DataHeader">DataHeader</a></code></li>
<li><code><a title="alfred.fm.remote.protos.query_pb2_grpc.QueryServiceServicer.DataReady" href="protos/query_pb2_grpc.html#alfred.fm.remote.protos.query_pb2_grpc.QueryServiceServicer.DataReady">DataReady</a></code></li>
<li><code><a title="alfred.fm.remote.protos.query_pb2_grpc.QueryServiceServicer.Inference" href="protos/query_pb2_grpc.html#alfred.fm.remote.protos.query_pb2_grpc.QueryServiceServicer.Inference">Inference</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="alfred.fm.remote" href="index.html">alfred.fm.remote</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="alfred.fm.remote.grpc.gRPCClient" href="#alfred.fm.remote.grpc.gRPCClient">gRPCClient</a></code></h4>
<ul class="">
<li><code><a title="alfred.fm.remote.grpc.gRPCClient.run" href="#alfred.fm.remote.grpc.gRPCClient.run">run</a></code></li>
<li><code><a title="alfred.fm.remote.grpc.gRPCClient.run_dataset" href="#alfred.fm.remote.grpc.gRPCClient.run_dataset">run_dataset</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="alfred.fm.remote.grpc.gRPCServer" href="#alfred.fm.remote.grpc.gRPCServer">gRPCServer</a></code></h4>
<ul class="">
<li><code><a title="alfred.fm.remote.grpc.gRPCServer.close" href="#alfred.fm.remote.grpc.gRPCServer.close">close</a></code></li>
<li><code><a title="alfred.fm.remote.grpc.gRPCServer.port_finder" href="#alfred.fm.remote.grpc.gRPCServer.port_finder">port_finder</a></code></li>
<li><code><a title="alfred.fm.remote.grpc.gRPCServer.restart" href="#alfred.fm.remote.grpc.gRPCServer.restart">restart</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>