<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>alfred.fm.utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>alfred.fm.utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import gc
import logging
from collections import OrderedDict
from typing import List, Union, Tuple, Optional

import numpy as np
import torch

from .query import Query, RankedQuery, CompletionQuery
from .response import RankedResponse

logger = logging.getLogger(__name__)

LMT_SIZE_FACTOR = 86381


def clear_cuda_cache():
    &#34;&#34;&#34;
    Clear cuda cache via garbage collection
    &#34;&#34;&#34;
    gc.collect()
    torch.cuda.empty_cache()


def normalize_logits(logits: torch.Tensor) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    Normalize raw logit scores from a foundation model.

    This function normalizes raw logit scores from a foundation model using the softmax function.
    Other normalization methods can be used in the future if needed.

    :param logits: The raw logit scores to be normalized.
    :type logits: torch.Tensor
    :return: The normalized logit scores.
    :rtype: torch.Tensor
    &#34;&#34;&#34;
    return torch.softmax(logits, dim=-1)


def reorder_array(arr: Union[np.ndarray, torch.Tensor, list],
                  order: Union[np.ndarray, torch.Tensor, list]) -&gt; Union[np.ndarray, torch.Tensor, list]:
    &#34;&#34;&#34;
    Reorder an array according to a given order.

    This function reorders the elements in an array according to the order specified by a separate array.

    :param arr: The array to be reordered. Can be a NumPy array, PyTorch tensor, or Python list.
    :type arr: Union[np.ndarray, torch.Tensor, list]
    :param order: The order array. Can be a NumPy array, PyTorch tensor, or Python list.
    :type order: Union[np.ndarray, torch.Tensor, list]
    :return: The reordered array. Has the same type as the input `arr`.
    :rtype: Union[np.ndarray, torch.Tensor, list]
    &#34;&#34;&#34;
    if isinstance(arr, list):
        arr = np.array(arr, object)
    if isinstance(order, list):
        order = np.array(order)
    return arr[order]


class DynamicBatcher:
    &#34;&#34;&#34;
    Dynamic Batching Utility
    Maximize GPU Utilization by batching queries of similar sizes
    &#34;&#34;&#34;

    def __init__(
            self,
            queries: Union[List[Query], List[str]],
            max_batch_size: int = 2048,
    ):
        &#34;&#34;&#34;
        Initialize a DynamicBatcher

        :param queries: A list of queries to be batched
        :type queries: Union[List[Query], List[str]]
        :param max_batch_size: The maximum batch size
        :type max_batch_size: int
        &#34;&#34;&#34;
        self.queries = queries
        self.max_batch_size = max_batch_size

        if torch.cuda.is_available():
            gpu_mem = torch.cuda.get_device_properties(0).total_memory
            free_mem = gpu_mem - torch.cuda.memory_allocated(0)
        else:
            free_mem = -1

        # Get Approximate Maximum Batch Size
        if free_mem &gt; 0:
            self.max_batch_size = min(int(free_mem / LMT_SIZE_FACTOR), max_batch_size)
        self.limit_size = LMT_SIZE_FACTOR
        self.ranked = False
        if isinstance(self.queries[0], RankedQuery):
            # Enforcing Uniform Candidate sizes and order across one set of
            # RankedQueries
            self.candidates = self.queries[0].candidates
            self.candidate_size = len(self.candidates)
            self.ranked = True

    def merge_rank_response(self,
                            responses: List[OrderedDict],
                            softmax: bool = True,
                            candidate_token_len: Union[List[int], int] = 1,
                            ) -&gt; RankedResponse:
        &#34;&#34;&#34;
        Merge a list of responses with raw logit into a single RankedResponse
        Assumption: Candidate Order is the same across all ranked queries

        :param responses: A list of responses to be merged
        :type responses: List[OrderedDict]
        :param softmax: Whether to apply softmax to the logits
        :type softmax: bool
        :param candidate_token_len: The length of the candidate in terms of tokens
        :type candidate_token_len: Union[List[int], int]
        :return: A merged response
        :rtype: RankedResponse
        &#34;&#34;&#34;
        if isinstance(candidate_token_len, int):
            candidate_token_len = [candidate_token_len] * len(self.candidates)

        scores = torch.empty(len(candidate_token_len))
        for response_idx, response in enumerate(responses):
            scores[response_idx] = response[&#39;logit&#39;]

        if softmax:
            scores = torch.nn.functional.softmax(scores, dim=0)
        pred = self.candidates[int(torch.argmax(scores, dim=0))]

        # TODO: For now we dont support gradient pass thru
        scores = {candidate: score.item()
                  for candidate, score in zip(self.candidates, scores)}

        return RankedResponse(prediction=pred, scores=scores, )

    def reorder(self,
                inst: List,
                offset: Optional[int] = None,
                candidate_token_len: Optional[Union[int,
                List[int]]] = None) -&gt; List:
        &#34;&#34;&#34;
        Reordering the responses according to the original order of the queries

        :param inst: The list of responses to be reordered
        :type inst: List
        :param offset: The offset of the responses
        :type offset: Optional[int]
        :param candidate_token_len: The length of the candidate in terms of tokens
        :type candidate_token_len: Optional[Union[int, List[int]]]
        :return: The reordered responses
        :rtype: List of responses
        &#34;&#34;&#34;

        if len(inst) != len(self.len_sorted_idx):
            if offset:
                _inst = np.empty([len(inst)])
                for idx, i in enumerate(
                        self.len_sorted_idx[offset:offset + len(inst)]):
                    _inst[idx] = inst[i]
                return list(_inst)
            raise ValueError(
                f&#34;Length of inst {len(inst)} does not match length of sorted index {len(self.len_sorted_idx)}&#34;)
        if self.len_sorted_idx is None:
            raise ValueError(&#34;Batching has not been performed yet&#34;)

        reordered_inst = reorder_array(inst, self.len_sorted_idx)

        if self.ranked:
            assert len(candidate_token_len) == len(
                self.candidates), f&#34;Length of candidate_token_len {len(candidate_token_len)} does not match length of candidates {len(self.candidates)}&#34;
            reordered_inst = [self.merge_rank_response(reordered_inst[i:i + self.candidate_size]) for i in
                              range(0, len(reordered_inst), self.candidate_size)]

        clear_cuda_cache()
        return list(reordered_inst)

    def batch(self) -&gt; List:
        &#39;&#39;&#39;
        Batch a list of instances into a list of batches.
        If the instances are of different sizes, they will be sorted by size
        and batched accordingly

        :return: A list of batches
        :rtype: List[List[Instance]]
        &#39;&#39;&#39;
        insts = []
        candidates = []
        for query in self.queries:
            if isinstance(query, str):
                insts.append(query)
            elif isinstance(query, CompletionQuery):
                insts += query.load()
            elif isinstance(query, RankedQuery):
                insts += [query.prompt] * len(query.candidates)
                candidates += query.candidates
            elif isinstance(query, Tuple):
                insts.append(query[0])
                candidates.append(query[1])
            else:
                logger.error(f&#34;Unknown query type {type(query)}&#34;)
                raise ValueError(f&#34;Input type {type(query)} not supported&#34;)

        ranked = len(candidates) &gt; 0
        if ranked:
            self.limit_size /= self.candidate_size
        inst_len = [len(inst) for inst in insts]
        inst_len_sorted, inst_len_sorted_idx = torch.sort(
            torch.tensor(inst_len), descending=True)

        self.len_sorted_idx = inst_len_sorted_idx

        curr_batch = []
        curr_sz = 0
        curr_max = -1
        curr_batch_sz = 0

        batches = []
        for sorted_idx, index in enumerate(inst_len_sorted_idx):
            inst_len = inst_len_sorted[sorted_idx]
            curr_inst = (insts[index], candidates[index]
                         ) if ranked else insts[index]
            if curr_sz &lt; self.limit_size and curr_batch_sz &lt; self.max_batch_size:
                curr_max = max(curr_max, inst_len)
                new_sz = curr_max * curr_batch_sz
                if new_sz &gt;= self.limit_size or curr_batch_sz &gt;= self.max_batch_size:
                    batches.append(curr_batch)
                    curr_batch = [curr_inst]
                    curr_sz = inst_len
                    curr_max = inst_len
                    curr_batch_sz = 1
                else:
                    curr_batch.append(curr_inst)
                    curr_batch_sz += 1
                    curr_sz = new_sz
            else:
                batches.append(curr_batch)
                curr_batch = [curr_inst]
                curr_sz = inst_len
                curr_max = inst_len
                curr_batch_sz = 1
        batches.append(curr_batch)

        clear_cuda_cache()

        return batches</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="alfred.fm.utils.clear_cuda_cache"><code class="name flex">
<span>def <span class="ident">clear_cuda_cache</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Clear cuda cache via garbage collection</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear_cuda_cache():
    &#34;&#34;&#34;
    Clear cuda cache via garbage collection
    &#34;&#34;&#34;
    gc.collect()
    torch.cuda.empty_cache()</code></pre>
</details>
</dd>
<dt id="alfred.fm.utils.normalize_logits"><code class="name flex">
<span>def <span class="ident">normalize_logits</span></span>(<span>logits: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Normalize raw logit scores from a foundation model.</p>
<p>This function normalizes raw logit scores from a foundation model using the softmax function.
Other normalization methods can be used in the future if needed.</p>
<p>:param logits: The raw logit scores to be normalized.
:type logits: torch.Tensor
:return: The normalized logit scores.
:rtype: torch.Tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize_logits(logits: torch.Tensor) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    Normalize raw logit scores from a foundation model.

    This function normalizes raw logit scores from a foundation model using the softmax function.
    Other normalization methods can be used in the future if needed.

    :param logits: The raw logit scores to be normalized.
    :type logits: torch.Tensor
    :return: The normalized logit scores.
    :rtype: torch.Tensor
    &#34;&#34;&#34;
    return torch.softmax(logits, dim=-1)</code></pre>
</details>
</dd>
<dt id="alfred.fm.utils.reorder_array"><code class="name flex">
<span>def <span class="ident">reorder_array</span></span>(<span>arr: Union[numpy.ndarray, torch.Tensor, list], order: Union[numpy.ndarray, torch.Tensor, list]) ‑> Union[numpy.ndarray, torch.Tensor, list]</span>
</code></dt>
<dd>
<div class="desc"><p>Reorder an array according to a given order.</p>
<p>This function reorders the elements in an array according to the order specified by a separate array.</p>
<p>:param arr: The array to be reordered. Can be a NumPy array, PyTorch tensor, or Python list.
:type arr: Union[np.ndarray, torch.Tensor, list]
:param order: The order array. Can be a NumPy array, PyTorch tensor, or Python list.
:type order: Union[np.ndarray, torch.Tensor, list]
:return: The reordered array. Has the same type as the input <code>arr</code>.
:rtype: Union[np.ndarray, torch.Tensor, list]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reorder_array(arr: Union[np.ndarray, torch.Tensor, list],
                  order: Union[np.ndarray, torch.Tensor, list]) -&gt; Union[np.ndarray, torch.Tensor, list]:
    &#34;&#34;&#34;
    Reorder an array according to a given order.

    This function reorders the elements in an array according to the order specified by a separate array.

    :param arr: The array to be reordered. Can be a NumPy array, PyTorch tensor, or Python list.
    :type arr: Union[np.ndarray, torch.Tensor, list]
    :param order: The order array. Can be a NumPy array, PyTorch tensor, or Python list.
    :type order: Union[np.ndarray, torch.Tensor, list]
    :return: The reordered array. Has the same type as the input `arr`.
    :rtype: Union[np.ndarray, torch.Tensor, list]
    &#34;&#34;&#34;
    if isinstance(arr, list):
        arr = np.array(arr, object)
    if isinstance(order, list):
        order = np.array(order)
    return arr[order]</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="alfred.fm.utils.DynamicBatcher"><code class="flex name class">
<span>class <span class="ident">DynamicBatcher</span></span>
<span>(</span><span>queries: Union[List[<a title="alfred.fm.query.query.Query" href="query/query.html#alfred.fm.query.query.Query">Query</a>], List[str]], max_batch_size: int = 2048)</span>
</code></dt>
<dd>
<div class="desc"><p>Dynamic Batching Utility
Maximize GPU Utilization by batching queries of similar sizes</p>
<p>Initialize a DynamicBatcher</p>
<p>:param queries: A list of queries to be batched
:type queries: Union[List[Query], List[str]]
:param max_batch_size: The maximum batch size
:type max_batch_size: int</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DynamicBatcher:
    &#34;&#34;&#34;
    Dynamic Batching Utility
    Maximize GPU Utilization by batching queries of similar sizes
    &#34;&#34;&#34;

    def __init__(
            self,
            queries: Union[List[Query], List[str]],
            max_batch_size: int = 2048,
    ):
        &#34;&#34;&#34;
        Initialize a DynamicBatcher

        :param queries: A list of queries to be batched
        :type queries: Union[List[Query], List[str]]
        :param max_batch_size: The maximum batch size
        :type max_batch_size: int
        &#34;&#34;&#34;
        self.queries = queries
        self.max_batch_size = max_batch_size

        if torch.cuda.is_available():
            gpu_mem = torch.cuda.get_device_properties(0).total_memory
            free_mem = gpu_mem - torch.cuda.memory_allocated(0)
        else:
            free_mem = -1

        # Get Approximate Maximum Batch Size
        if free_mem &gt; 0:
            self.max_batch_size = min(int(free_mem / LMT_SIZE_FACTOR), max_batch_size)
        self.limit_size = LMT_SIZE_FACTOR
        self.ranked = False
        if isinstance(self.queries[0], RankedQuery):
            # Enforcing Uniform Candidate sizes and order across one set of
            # RankedQueries
            self.candidates = self.queries[0].candidates
            self.candidate_size = len(self.candidates)
            self.ranked = True

    def merge_rank_response(self,
                            responses: List[OrderedDict],
                            softmax: bool = True,
                            candidate_token_len: Union[List[int], int] = 1,
                            ) -&gt; RankedResponse:
        &#34;&#34;&#34;
        Merge a list of responses with raw logit into a single RankedResponse
        Assumption: Candidate Order is the same across all ranked queries

        :param responses: A list of responses to be merged
        :type responses: List[OrderedDict]
        :param softmax: Whether to apply softmax to the logits
        :type softmax: bool
        :param candidate_token_len: The length of the candidate in terms of tokens
        :type candidate_token_len: Union[List[int], int]
        :return: A merged response
        :rtype: RankedResponse
        &#34;&#34;&#34;
        if isinstance(candidate_token_len, int):
            candidate_token_len = [candidate_token_len] * len(self.candidates)

        scores = torch.empty(len(candidate_token_len))
        for response_idx, response in enumerate(responses):
            scores[response_idx] = response[&#39;logit&#39;]

        if softmax:
            scores = torch.nn.functional.softmax(scores, dim=0)
        pred = self.candidates[int(torch.argmax(scores, dim=0))]

        # TODO: For now we dont support gradient pass thru
        scores = {candidate: score.item()
                  for candidate, score in zip(self.candidates, scores)}

        return RankedResponse(prediction=pred, scores=scores, )

    def reorder(self,
                inst: List,
                offset: Optional[int] = None,
                candidate_token_len: Optional[Union[int,
                List[int]]] = None) -&gt; List:
        &#34;&#34;&#34;
        Reordering the responses according to the original order of the queries

        :param inst: The list of responses to be reordered
        :type inst: List
        :param offset: The offset of the responses
        :type offset: Optional[int]
        :param candidate_token_len: The length of the candidate in terms of tokens
        :type candidate_token_len: Optional[Union[int, List[int]]]
        :return: The reordered responses
        :rtype: List of responses
        &#34;&#34;&#34;

        if len(inst) != len(self.len_sorted_idx):
            if offset:
                _inst = np.empty([len(inst)])
                for idx, i in enumerate(
                        self.len_sorted_idx[offset:offset + len(inst)]):
                    _inst[idx] = inst[i]
                return list(_inst)
            raise ValueError(
                f&#34;Length of inst {len(inst)} does not match length of sorted index {len(self.len_sorted_idx)}&#34;)
        if self.len_sorted_idx is None:
            raise ValueError(&#34;Batching has not been performed yet&#34;)

        reordered_inst = reorder_array(inst, self.len_sorted_idx)

        if self.ranked:
            assert len(candidate_token_len) == len(
                self.candidates), f&#34;Length of candidate_token_len {len(candidate_token_len)} does not match length of candidates {len(self.candidates)}&#34;
            reordered_inst = [self.merge_rank_response(reordered_inst[i:i + self.candidate_size]) for i in
                              range(0, len(reordered_inst), self.candidate_size)]

        clear_cuda_cache()
        return list(reordered_inst)

    def batch(self) -&gt; List:
        &#39;&#39;&#39;
        Batch a list of instances into a list of batches.
        If the instances are of different sizes, they will be sorted by size
        and batched accordingly

        :return: A list of batches
        :rtype: List[List[Instance]]
        &#39;&#39;&#39;
        insts = []
        candidates = []
        for query in self.queries:
            if isinstance(query, str):
                insts.append(query)
            elif isinstance(query, CompletionQuery):
                insts += query.load()
            elif isinstance(query, RankedQuery):
                insts += [query.prompt] * len(query.candidates)
                candidates += query.candidates
            elif isinstance(query, Tuple):
                insts.append(query[0])
                candidates.append(query[1])
            else:
                logger.error(f&#34;Unknown query type {type(query)}&#34;)
                raise ValueError(f&#34;Input type {type(query)} not supported&#34;)

        ranked = len(candidates) &gt; 0
        if ranked:
            self.limit_size /= self.candidate_size
        inst_len = [len(inst) for inst in insts]
        inst_len_sorted, inst_len_sorted_idx = torch.sort(
            torch.tensor(inst_len), descending=True)

        self.len_sorted_idx = inst_len_sorted_idx

        curr_batch = []
        curr_sz = 0
        curr_max = -1
        curr_batch_sz = 0

        batches = []
        for sorted_idx, index in enumerate(inst_len_sorted_idx):
            inst_len = inst_len_sorted[sorted_idx]
            curr_inst = (insts[index], candidates[index]
                         ) if ranked else insts[index]
            if curr_sz &lt; self.limit_size and curr_batch_sz &lt; self.max_batch_size:
                curr_max = max(curr_max, inst_len)
                new_sz = curr_max * curr_batch_sz
                if new_sz &gt;= self.limit_size or curr_batch_sz &gt;= self.max_batch_size:
                    batches.append(curr_batch)
                    curr_batch = [curr_inst]
                    curr_sz = inst_len
                    curr_max = inst_len
                    curr_batch_sz = 1
                else:
                    curr_batch.append(curr_inst)
                    curr_batch_sz += 1
                    curr_sz = new_sz
            else:
                batches.append(curr_batch)
                curr_batch = [curr_inst]
                curr_sz = inst_len
                curr_max = inst_len
                curr_batch_sz = 1
        batches.append(curr_batch)

        clear_cuda_cache()

        return batches</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="alfred.fm.utils.DynamicBatcher.batch"><code class="name flex">
<span>def <span class="ident">batch</span></span>(<span>self) ‑> List[~T]</span>
</code></dt>
<dd>
<div class="desc"><p>Batch a list of instances into a list of batches.
If the instances are of different sizes, they will be sorted by size
and batched accordingly</p>
<p>:return: A list of batches
:rtype: List[List[Instance]]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch(self) -&gt; List:
    &#39;&#39;&#39;
    Batch a list of instances into a list of batches.
    If the instances are of different sizes, they will be sorted by size
    and batched accordingly

    :return: A list of batches
    :rtype: List[List[Instance]]
    &#39;&#39;&#39;
    insts = []
    candidates = []
    for query in self.queries:
        if isinstance(query, str):
            insts.append(query)
        elif isinstance(query, CompletionQuery):
            insts += query.load()
        elif isinstance(query, RankedQuery):
            insts += [query.prompt] * len(query.candidates)
            candidates += query.candidates
        elif isinstance(query, Tuple):
            insts.append(query[0])
            candidates.append(query[1])
        else:
            logger.error(f&#34;Unknown query type {type(query)}&#34;)
            raise ValueError(f&#34;Input type {type(query)} not supported&#34;)

    ranked = len(candidates) &gt; 0
    if ranked:
        self.limit_size /= self.candidate_size
    inst_len = [len(inst) for inst in insts]
    inst_len_sorted, inst_len_sorted_idx = torch.sort(
        torch.tensor(inst_len), descending=True)

    self.len_sorted_idx = inst_len_sorted_idx

    curr_batch = []
    curr_sz = 0
    curr_max = -1
    curr_batch_sz = 0

    batches = []
    for sorted_idx, index in enumerate(inst_len_sorted_idx):
        inst_len = inst_len_sorted[sorted_idx]
        curr_inst = (insts[index], candidates[index]
                     ) if ranked else insts[index]
        if curr_sz &lt; self.limit_size and curr_batch_sz &lt; self.max_batch_size:
            curr_max = max(curr_max, inst_len)
            new_sz = curr_max * curr_batch_sz
            if new_sz &gt;= self.limit_size or curr_batch_sz &gt;= self.max_batch_size:
                batches.append(curr_batch)
                curr_batch = [curr_inst]
                curr_sz = inst_len
                curr_max = inst_len
                curr_batch_sz = 1
            else:
                curr_batch.append(curr_inst)
                curr_batch_sz += 1
                curr_sz = new_sz
        else:
            batches.append(curr_batch)
            curr_batch = [curr_inst]
            curr_sz = inst_len
            curr_max = inst_len
            curr_batch_sz = 1
    batches.append(curr_batch)

    clear_cuda_cache()

    return batches</code></pre>
</details>
</dd>
<dt id="alfred.fm.utils.DynamicBatcher.merge_rank_response"><code class="name flex">
<span>def <span class="ident">merge_rank_response</span></span>(<span>self, responses: List[collections.OrderedDict], softmax: bool = True, candidate_token_len: Union[List[int], int] = 1) ‑> <a title="alfred.fm.response.ranked_response.RankedResponse" href="response/ranked_response.html#alfred.fm.response.ranked_response.RankedResponse">RankedResponse</a></span>
</code></dt>
<dd>
<div class="desc"><p>Merge a list of responses with raw logit into a single RankedResponse
Assumption: Candidate Order is the same across all ranked queries</p>
<p>:param responses: A list of responses to be merged
:type responses: List[OrderedDict]
:param softmax: Whether to apply softmax to the logits
:type softmax: bool
:param candidate_token_len: The length of the candidate in terms of tokens
:type candidate_token_len: Union[List[int], int]
:return: A merged response
:rtype: RankedResponse</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_rank_response(self,
                        responses: List[OrderedDict],
                        softmax: bool = True,
                        candidate_token_len: Union[List[int], int] = 1,
                        ) -&gt; RankedResponse:
    &#34;&#34;&#34;
    Merge a list of responses with raw logit into a single RankedResponse
    Assumption: Candidate Order is the same across all ranked queries

    :param responses: A list of responses to be merged
    :type responses: List[OrderedDict]
    :param softmax: Whether to apply softmax to the logits
    :type softmax: bool
    :param candidate_token_len: The length of the candidate in terms of tokens
    :type candidate_token_len: Union[List[int], int]
    :return: A merged response
    :rtype: RankedResponse
    &#34;&#34;&#34;
    if isinstance(candidate_token_len, int):
        candidate_token_len = [candidate_token_len] * len(self.candidates)

    scores = torch.empty(len(candidate_token_len))
    for response_idx, response in enumerate(responses):
        scores[response_idx] = response[&#39;logit&#39;]

    if softmax:
        scores = torch.nn.functional.softmax(scores, dim=0)
    pred = self.candidates[int(torch.argmax(scores, dim=0))]

    # TODO: For now we dont support gradient pass thru
    scores = {candidate: score.item()
              for candidate, score in zip(self.candidates, scores)}

    return RankedResponse(prediction=pred, scores=scores, )</code></pre>
</details>
</dd>
<dt id="alfred.fm.utils.DynamicBatcher.reorder"><code class="name flex">
<span>def <span class="ident">reorder</span></span>(<span>self, inst: List[~T], offset: Optional[int] = None, candidate_token_len: Union[int, List[int], None] = None) ‑> List[~T]</span>
</code></dt>
<dd>
<div class="desc"><p>Reordering the responses according to the original order of the queries</p>
<p>:param inst: The list of responses to be reordered
:type inst: List
:param offset: The offset of the responses
:type offset: Optional[int]
:param candidate_token_len: The length of the candidate in terms of tokens
:type candidate_token_len: Optional[Union[int, List[int]]]
:return: The reordered responses
:rtype: List of responses</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reorder(self,
            inst: List,
            offset: Optional[int] = None,
            candidate_token_len: Optional[Union[int,
            List[int]]] = None) -&gt; List:
    &#34;&#34;&#34;
    Reordering the responses according to the original order of the queries

    :param inst: The list of responses to be reordered
    :type inst: List
    :param offset: The offset of the responses
    :type offset: Optional[int]
    :param candidate_token_len: The length of the candidate in terms of tokens
    :type candidate_token_len: Optional[Union[int, List[int]]]
    :return: The reordered responses
    :rtype: List of responses
    &#34;&#34;&#34;

    if len(inst) != len(self.len_sorted_idx):
        if offset:
            _inst = np.empty([len(inst)])
            for idx, i in enumerate(
                    self.len_sorted_idx[offset:offset + len(inst)]):
                _inst[idx] = inst[i]
            return list(_inst)
        raise ValueError(
            f&#34;Length of inst {len(inst)} does not match length of sorted index {len(self.len_sorted_idx)}&#34;)
    if self.len_sorted_idx is None:
        raise ValueError(&#34;Batching has not been performed yet&#34;)

    reordered_inst = reorder_array(inst, self.len_sorted_idx)

    if self.ranked:
        assert len(candidate_token_len) == len(
            self.candidates), f&#34;Length of candidate_token_len {len(candidate_token_len)} does not match length of candidates {len(self.candidates)}&#34;
        reordered_inst = [self.merge_rank_response(reordered_inst[i:i + self.candidate_size]) for i in
                          range(0, len(reordered_inst), self.candidate_size)]

    clear_cuda_cache()
    return list(reordered_inst)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="alfred.fm" href="index.html">alfred.fm</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="alfred.fm.utils.clear_cuda_cache" href="#alfred.fm.utils.clear_cuda_cache">clear_cuda_cache</a></code></li>
<li><code><a title="alfred.fm.utils.normalize_logits" href="#alfred.fm.utils.normalize_logits">normalize_logits</a></code></li>
<li><code><a title="alfred.fm.utils.reorder_array" href="#alfred.fm.utils.reorder_array">reorder_array</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="alfred.fm.utils.DynamicBatcher" href="#alfred.fm.utils.DynamicBatcher">DynamicBatcher</a></code></h4>
<ul class="">
<li><code><a title="alfred.fm.utils.DynamicBatcher.batch" href="#alfred.fm.utils.DynamicBatcher.batch">batch</a></code></li>
<li><code><a title="alfred.fm.utils.DynamicBatcher.merge_rank_response" href="#alfred.fm.utils.DynamicBatcher.merge_rank_response">merge_rank_response</a></code></li>
<li><code><a title="alfred.fm.utils.DynamicBatcher.reorder" href="#alfred.fm.utils.DynamicBatcher.reorder">reorder</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>