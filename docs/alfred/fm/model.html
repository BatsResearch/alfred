<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>alfred.fm.model API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>alfred.fm.model</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import abc
import logging
import os
from contextlib import nullcontext
from typing import List, Optional, Dict, Union, Tuple, OrderedDict, Any

import numpy as np
import torch
from tqdm.auto import tqdm

from .query import Query, RankedQuery, CompletionQuery
from .response import Response, CompletionResponse, RankedResponse
from .utils import DynamicBatcher

logger = logging.getLogger(__name__)


class FoundationModel(abc.ABC):
    &#34;&#34;&#34;
    Generic interface for foundation model class
    &#34;&#34;&#34;

    @abc.abstractmethod
    def _generate_batch(self,
                        batch_instance: Union[List[str]],
                        **kwargs,
                        ) -&gt; List[Response]:
        &#34;&#34;&#34;
        For completing / generating given a batch of queries
        Run a batch of queries through the foundation model

        :param batch_instance: A batch of query objects or raw query content (e.g. string or embedding arrays)
        :type batch_instance: Union[List[CompletionQuery], List[str]]
        :param kwargs: Additional arguments to pass to the foundation model
        :type batch_instance: Union[List[CompletionQuery], List[str]]
        :return: A list of responses
        :rtype List[Response]
        &#34;&#34;&#34;
        raise NotImplementedError(
            f&#34;_infer_batch() is not implemented for {self.__class__.__name__}&#34;)

    @abc.abstractmethod
    def _score_batch(self,
                     batch_instance: Union[List[Tuple[str, str]], List[str]],
                     **kwargs,
                     ) -&gt; List[Response]:
        &#34;&#34;&#34;
        For scoring / ranking candidate queries.
        Run a batch of queries through the foundation model.

        :param batch_instance: A batch of query objects or raw query content (e.g. string or embedding arrays)
        :type batch_instance: Union[List[RankedQuery], List[str]]
        :return: A list of responses
        :rtype List[Response]
        &#34;&#34;&#34;
        raise NotImplementedError(
            f&#34;_score_batch() is not implemented for {self.__class__.__name__}&#34;)

    def forward(self,
                queries: Union[List[Query],
                List[str],
                List[Tuple[str, str]]],
                batch_policy: str = &#39;dynamic&#39;,
                batch_size: int = 1024,
                score: bool = False,
                **kwargs,
                ) -&gt; Union[List[CompletionResponse],
    List[RankedResponse],
    List[OrderedDict]]:
        &#34;&#34;&#34;
        This function is the main entry point for running queries through the foundation model.
        It accepts raw query content and automatically converts it into query objects.
        The function then determines whether to run the queries through the _generate_batch
        or _score_batch method based on the type of queries. Finally, the function processes
        the queries using one of two batching policies (dynamic, static) and passes them
        through the foundation model.

        :param queries: A list of queries
        :type queries: Union[List[Query], List[str], List[Tuple[str, str]]]
        :param batch_policy: The batching policy to use. Can be either &#39;dynamic&#39; or &#39;static&#39;
        :type batch_policy: str
        :param batch_size: The batch size to use for static batching or maximum batch size for dynamic batching
        :type batch_size: int
        :param score: Whether to run the queries through the _score_batch() method
        :type score: bool
        :param kwargs: Additional arguments to pass to the foundation model
        :type kwargs: Any
        :return: A list of responses
        :rtype: Union[List[CompletionResponse], List[RankedResponse], List[OrderedDict]]
        &#34;&#34;&#34;

        with_grad = kwargs.get(&#39;with_grad&#39;, False)
        no_tqdm = kwargs.get(&#39;no_tqdm&#39;, False)

        if type(queries[0]) in [RankedQuery, tuple]:
            score = True

        if batch_policy == &#39;static&#39;:
            # To near equally sized batches
            batched_queries = np.array_split(
                queries, len(queries) // batch_size
            )
        elif batch_policy == &#39;dynamic&#39;:
            DB = DynamicBatcher(queries, max_batch_size=batch_size)
            batched_queries = DB.batch()
        else:
            raise ValueError(f&#34;batch_policy {batch_policy} not supported&#34;)

        inferece_fn = self._score_batch if score else self._generate_batch
        logger.log(logging.INFO, f&#34;Inferring {len(batched_queries)} batches&#34;)

        with nullcontext() if with_grad else torch.no_grad():
            responses = []
            for batch_id, batch in enumerate(
                    tqdm(batched_queries, disable=no_tqdm)):
                responses += inferece_fn(batch, **kwargs)

        if score:
            # Assuming candidates are the same for all queries for one
            # run/batch
            try:
                candidate_token_len = [
                    len(x) for x in self.model.tokenizer(
                        list(
                            queries[0].candidates),
                        padding=True,
                        add_special_tokens=False).input_ids]
            except BaseException:
                logger.info(
                    &#34;Unable to get candidate token length, defaulting to token length of 1&#34;)
                candidate_token_len = [1] * len(queries[0].candidates)
        else:
            candidate_token_len = None

        if batch_policy == &#39;dynamic&#39;:
            responses = DB.reorder(
                responses, candidate_token_len=candidate_token_len)

        return list(responses)

    def generate(self,
                 queries: Union[List[CompletionQuery], List[str]],
                 batch_policy: str = &#39;dynamic&#39;,
                 batch_size: int = 1024,
                 **kwargs,
                 ) -&gt; List[CompletionResponse]:
        &#34;&#34;&#34;
        This function is a wrapper around the forward function for running
        CompletionQuery objects through the foundation model. It returns a list
        of CompletionResponse objects.

        :param queries:  A list of CompletionQuery or raw query content (as string)
        :type queries: Union[List[CompletionQuery], List[str]]
        :param batch_policy: The batching policy to use. Can be either &#39;dynamic&#39; or &#39;static&#39;
        :type batch_policy: str
        :param batch_size: The batch size to use for static batching or maximum batch size for dynamic batching
        :type batch_size: int
        :param kwargs: Additional arguments to pass to the foundation model
        :type kwargs: Any
        :return: A list of CompletionResponse
        :rtype: List[CompletionResponse]
        &#34;&#34;&#34;
        return self.forward(queries, batch_policy, batch_size, **kwargs)

    def score(self,
              queries: List[RankedQuery],
              batch_policy: str = &#39;dynamic&#39;,
              batch_size: int = 1024,
              **kwargs: Any,
              ) -&gt; List[RankedResponse]:
        &#34;&#34;&#34;
        This function is a wrapper around the forward function
        for running RankedQuery objects through the foundation model.
        It returns a list of RankedResponse objects.

        :param queries:  A list of RankedQuery
        :type queries: List[RankedQuery]
        :param batch_policy: The batching policy to use. Can be either &#39;dynamic&#39; or &#39;static&#39;
        :type batch_policy: str
        :param batch_size: The batch size to use for static batching or maximum batch size for dynamic batching
        :type batch_size: int
        :param kwargs: Additional arguments to pass to the foundation model
        :type kwargs: Any
        :return: A list of RankedResponse
        :rtype: List[RankedResponse]
        &#34;&#34;&#34;

        return self.forward(
            queries,
            batch_policy,
            batch_size,
            score=True,
            **kwargs)

    def run(self,
            queries: Union[Query, str, Tuple[str, str], List[Query], List[str]],
            **kwargs: Any,
            ) -&gt; Union[str, Response, List[Response]]:
        &#34;&#34;&#34;
        This function is the main entry point for users to run queries through the foundation model.
        It accepts raw query content and automatically converts it into query objects.
        The function then processes the queries and returns the responses in the appropriate format.
        For single instance queries, a single response object is returned.

        :param queries: A single query or a list of queries
        :type queries: Union[Query, str, Tuple[str, str], List[Query], List[str]]
        :param kwargs: Additional arguments to pass to the foundation model
        :type kwargs: Any
        :return: A single response or a list of responses
        :rtype: Union[str, Response, List[Response]]
        &#34;&#34;&#34;
        if isinstance(queries, list):
            if type(queries[0]) in [tuple, RankedQuery]:
                score = True
            elif type(queries[0]) in [str, CompletionQuery]:
                score = False
            else:
                raise ValueError(f&#34;Unsupported query type {type(queries[0])}&#34;)
            return self.forward(queries, score=score, **kwargs)
        elif isinstance(queries, RankedQuery) or isinstance(queries, tuple):
            return self.forward([queries], score=True, **kwargs)[0]
        elif isinstance(queries, CompletionQuery) or isinstance(queries, str):
            return self._generate_batch(queries.load() if isinstance(
                queries, CompletionQuery) else [queries], **kwargs)[0]
        else:
            logger.warning(
                f&#34;Unsupported query type {type(queries)}, Attempting to run&#34;)
            return self.forward(queries, **kwargs)

    def __call__(self,
                 queries: Union[Query, str, Tuple[str, str], List[Query], List[str]],
                 **kwargs: Any,
                 ) -&gt; Union[str, Response, List[Response]]:
        &#34;&#34;&#34;
        This function returns the output of the run function when the
         model is called as a function. It can be used as model(queries),
         which is equivalent to model.run(queries).

        :param queries: A single query or a list of queries
        :type queries: Union[Query, str, dict, List[Query], List[str], List[dict]]
        :param kwargs: Additional arguments to pass to the foundation model
        :type kwargs: Any
        :return: A single response or a list of responses
        :rtype: Union[str, Response, List[Response]]
        &#34;&#34;&#34;

        return self.run(queries)


class APIAccessFoundationModel(FoundationModel):
    def __init__(self,
                 model_string: str,
                 cfg: Optional[Dict] = None):
        &#34;&#34;&#34;
        Initializes the APIAccessFoundationModel class,
        which wraps API-based models such as OpenAI GPT-3, Cohere, and AI21 etc.

        :param model_string: The model string to use (e.g. text-davinci-003 for OpenAI GPT-3)
        :type model_string: str
        :param cfg: (optional) A dictionary containing configuration options for the model
        :type cfg: Dict
        &#34;&#34;&#34;
        self.cfg = cfg
        self.model_string = model_string


class LocalAccessFoundationModel(FoundationModel):
    def __init__(self,
                 model_string: str,
                 local_path: Optional[str] = None):
        &#34;&#34;&#34;
        Initializes the LocalAccessFoundationModel class,
         which wraps a local serving model such as PyTorch or HuggingFace.

        :param model_string: The model string that defines the model type
                                (e.g. gpt2 for HuggingFace GPT-2)
        :type model_string: str
        :param local_path: (optional) The path to the local model directories.
               If not provided, the default path ~/.model_cache/{model_string} will be used.
        :type local_path: str
        &#34;&#34;&#34;
        self.local_path = local_path
        if local_path is None:
            self.local_path = os.path.join(
                os.path.expanduser(&#39;~&#39;), &#39;.model_cache&#39;, model_string)
        self.model_string = model_string</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="alfred.fm.model.APIAccessFoundationModel"><code class="flex name class">
<span>class <span class="ident">APIAccessFoundationModel</span></span>
<span>(</span><span>model_string: str, cfg: Optional[Dict[~KT, ~VT]] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Generic interface for foundation model class</p>
<p>Initializes the APIAccessFoundationModel class,
which wraps API-based models such as OpenAI GPT-3, Cohere, and AI21 etc.</p>
<p>:param model_string: The model string to use (e.g. text-davinci-003 for OpenAI GPT-3)
:type model_string: str
:param cfg: (optional) A dictionary containing configuration options for the model
:type cfg: Dict</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class APIAccessFoundationModel(FoundationModel):
    def __init__(self,
                 model_string: str,
                 cfg: Optional[Dict] = None):
        &#34;&#34;&#34;
        Initializes the APIAccessFoundationModel class,
        which wraps API-based models such as OpenAI GPT-3, Cohere, and AI21 etc.

        :param model_string: The model string to use (e.g. text-davinci-003 for OpenAI GPT-3)
        :type model_string: str
        :param cfg: (optional) A dictionary containing configuration options for the model
        :type cfg: Dict
        &#34;&#34;&#34;
        self.cfg = cfg
        self.model_string = model_string</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="alfred.fm.model.FoundationModel" href="#alfred.fm.model.FoundationModel">FoundationModel</a></li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="alfred.fm.openai.OpenAIModel" href="openai.html#alfred.fm.openai.OpenAIModel">OpenAIModel</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="alfred.fm.model.FoundationModel" href="#alfred.fm.model.FoundationModel">FoundationModel</a></b></code>:
<ul class="hlist">
<li><code><a title="alfred.fm.model.FoundationModel.forward" href="#alfred.fm.model.FoundationModel.forward">forward</a></code></li>
<li><code><a title="alfred.fm.model.FoundationModel.generate" href="#alfred.fm.model.FoundationModel.generate">generate</a></code></li>
<li><code><a title="alfred.fm.model.FoundationModel.run" href="#alfred.fm.model.FoundationModel.run">run</a></code></li>
<li><code><a title="alfred.fm.model.FoundationModel.score" href="#alfred.fm.model.FoundationModel.score">score</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="alfred.fm.model.FoundationModel"><code class="flex name class">
<span>class <span class="ident">FoundationModel</span></span>
</code></dt>
<dd>
<div class="desc"><p>Generic interface for foundation model class</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FoundationModel(abc.ABC):
    &#34;&#34;&#34;
    Generic interface for foundation model class
    &#34;&#34;&#34;

    @abc.abstractmethod
    def _generate_batch(self,
                        batch_instance: Union[List[str]],
                        **kwargs,
                        ) -&gt; List[Response]:
        &#34;&#34;&#34;
        For completing / generating given a batch of queries
        Run a batch of queries through the foundation model

        :param batch_instance: A batch of query objects or raw query content (e.g. string or embedding arrays)
        :type batch_instance: Union[List[CompletionQuery], List[str]]
        :param kwargs: Additional arguments to pass to the foundation model
        :type batch_instance: Union[List[CompletionQuery], List[str]]
        :return: A list of responses
        :rtype List[Response]
        &#34;&#34;&#34;
        raise NotImplementedError(
            f&#34;_infer_batch() is not implemented for {self.__class__.__name__}&#34;)

    @abc.abstractmethod
    def _score_batch(self,
                     batch_instance: Union[List[Tuple[str, str]], List[str]],
                     **kwargs,
                     ) -&gt; List[Response]:
        &#34;&#34;&#34;
        For scoring / ranking candidate queries.
        Run a batch of queries through the foundation model.

        :param batch_instance: A batch of query objects or raw query content (e.g. string or embedding arrays)
        :type batch_instance: Union[List[RankedQuery], List[str]]
        :return: A list of responses
        :rtype List[Response]
        &#34;&#34;&#34;
        raise NotImplementedError(
            f&#34;_score_batch() is not implemented for {self.__class__.__name__}&#34;)

    def forward(self,
                queries: Union[List[Query],
                List[str],
                List[Tuple[str, str]]],
                batch_policy: str = &#39;dynamic&#39;,
                batch_size: int = 1024,
                score: bool = False,
                **kwargs,
                ) -&gt; Union[List[CompletionResponse],
    List[RankedResponse],
    List[OrderedDict]]:
        &#34;&#34;&#34;
        This function is the main entry point for running queries through the foundation model.
        It accepts raw query content and automatically converts it into query objects.
        The function then determines whether to run the queries through the _generate_batch
        or _score_batch method based on the type of queries. Finally, the function processes
        the queries using one of two batching policies (dynamic, static) and passes them
        through the foundation model.

        :param queries: A list of queries
        :type queries: Union[List[Query], List[str], List[Tuple[str, str]]]
        :param batch_policy: The batching policy to use. Can be either &#39;dynamic&#39; or &#39;static&#39;
        :type batch_policy: str
        :param batch_size: The batch size to use for static batching or maximum batch size for dynamic batching
        :type batch_size: int
        :param score: Whether to run the queries through the _score_batch() method
        :type score: bool
        :param kwargs: Additional arguments to pass to the foundation model
        :type kwargs: Any
        :return: A list of responses
        :rtype: Union[List[CompletionResponse], List[RankedResponse], List[OrderedDict]]
        &#34;&#34;&#34;

        with_grad = kwargs.get(&#39;with_grad&#39;, False)
        no_tqdm = kwargs.get(&#39;no_tqdm&#39;, False)

        if type(queries[0]) in [RankedQuery, tuple]:
            score = True

        if batch_policy == &#39;static&#39;:
            # To near equally sized batches
            batched_queries = np.array_split(
                queries, len(queries) // batch_size
            )
        elif batch_policy == &#39;dynamic&#39;:
            DB = DynamicBatcher(queries, max_batch_size=batch_size)
            batched_queries = DB.batch()
        else:
            raise ValueError(f&#34;batch_policy {batch_policy} not supported&#34;)

        inferece_fn = self._score_batch if score else self._generate_batch
        logger.log(logging.INFO, f&#34;Inferring {len(batched_queries)} batches&#34;)

        with nullcontext() if with_grad else torch.no_grad():
            responses = []
            for batch_id, batch in enumerate(
                    tqdm(batched_queries, disable=no_tqdm)):
                responses += inferece_fn(batch, **kwargs)

        if score:
            # Assuming candidates are the same for all queries for one
            # run/batch
            try:
                candidate_token_len = [
                    len(x) for x in self.model.tokenizer(
                        list(
                            queries[0].candidates),
                        padding=True,
                        add_special_tokens=False).input_ids]
            except BaseException:
                logger.info(
                    &#34;Unable to get candidate token length, defaulting to token length of 1&#34;)
                candidate_token_len = [1] * len(queries[0].candidates)
        else:
            candidate_token_len = None

        if batch_policy == &#39;dynamic&#39;:
            responses = DB.reorder(
                responses, candidate_token_len=candidate_token_len)

        return list(responses)

    def generate(self,
                 queries: Union[List[CompletionQuery], List[str]],
                 batch_policy: str = &#39;dynamic&#39;,
                 batch_size: int = 1024,
                 **kwargs,
                 ) -&gt; List[CompletionResponse]:
        &#34;&#34;&#34;
        This function is a wrapper around the forward function for running
        CompletionQuery objects through the foundation model. It returns a list
        of CompletionResponse objects.

        :param queries:  A list of CompletionQuery or raw query content (as string)
        :type queries: Union[List[CompletionQuery], List[str]]
        :param batch_policy: The batching policy to use. Can be either &#39;dynamic&#39; or &#39;static&#39;
        :type batch_policy: str
        :param batch_size: The batch size to use for static batching or maximum batch size for dynamic batching
        :type batch_size: int
        :param kwargs: Additional arguments to pass to the foundation model
        :type kwargs: Any
        :return: A list of CompletionResponse
        :rtype: List[CompletionResponse]
        &#34;&#34;&#34;
        return self.forward(queries, batch_policy, batch_size, **kwargs)

    def score(self,
              queries: List[RankedQuery],
              batch_policy: str = &#39;dynamic&#39;,
              batch_size: int = 1024,
              **kwargs: Any,
              ) -&gt; List[RankedResponse]:
        &#34;&#34;&#34;
        This function is a wrapper around the forward function
        for running RankedQuery objects through the foundation model.
        It returns a list of RankedResponse objects.

        :param queries:  A list of RankedQuery
        :type queries: List[RankedQuery]
        :param batch_policy: The batching policy to use. Can be either &#39;dynamic&#39; or &#39;static&#39;
        :type batch_policy: str
        :param batch_size: The batch size to use for static batching or maximum batch size for dynamic batching
        :type batch_size: int
        :param kwargs: Additional arguments to pass to the foundation model
        :type kwargs: Any
        :return: A list of RankedResponse
        :rtype: List[RankedResponse]
        &#34;&#34;&#34;

        return self.forward(
            queries,
            batch_policy,
            batch_size,
            score=True,
            **kwargs)

    def run(self,
            queries: Union[Query, str, Tuple[str, str], List[Query], List[str]],
            **kwargs: Any,
            ) -&gt; Union[str, Response, List[Response]]:
        &#34;&#34;&#34;
        This function is the main entry point for users to run queries through the foundation model.
        It accepts raw query content and automatically converts it into query objects.
        The function then processes the queries and returns the responses in the appropriate format.
        For single instance queries, a single response object is returned.

        :param queries: A single query or a list of queries
        :type queries: Union[Query, str, Tuple[str, str], List[Query], List[str]]
        :param kwargs: Additional arguments to pass to the foundation model
        :type kwargs: Any
        :return: A single response or a list of responses
        :rtype: Union[str, Response, List[Response]]
        &#34;&#34;&#34;
        if isinstance(queries, list):
            if type(queries[0]) in [tuple, RankedQuery]:
                score = True
            elif type(queries[0]) in [str, CompletionQuery]:
                score = False
            else:
                raise ValueError(f&#34;Unsupported query type {type(queries[0])}&#34;)
            return self.forward(queries, score=score, **kwargs)
        elif isinstance(queries, RankedQuery) or isinstance(queries, tuple):
            return self.forward([queries], score=True, **kwargs)[0]
        elif isinstance(queries, CompletionQuery) or isinstance(queries, str):
            return self._generate_batch(queries.load() if isinstance(
                queries, CompletionQuery) else [queries], **kwargs)[0]
        else:
            logger.warning(
                f&#34;Unsupported query type {type(queries)}, Attempting to run&#34;)
            return self.forward(queries, **kwargs)

    def __call__(self,
                 queries: Union[Query, str, Tuple[str, str], List[Query], List[str]],
                 **kwargs: Any,
                 ) -&gt; Union[str, Response, List[Response]]:
        &#34;&#34;&#34;
        This function returns the output of the run function when the
         model is called as a function. It can be used as model(queries),
         which is equivalent to model.run(queries).

        :param queries: A single query or a list of queries
        :type queries: Union[Query, str, dict, List[Query], List[str], List[dict]]
        :param kwargs: Additional arguments to pass to the foundation model
        :type kwargs: Any
        :return: A single response or a list of responses
        :rtype: Union[str, Response, List[Response]]
        &#34;&#34;&#34;

        return self.run(queries)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="alfred.fm.model.APIAccessFoundationModel" href="#alfred.fm.model.APIAccessFoundationModel">APIAccessFoundationModel</a></li>
<li><a title="alfred.fm.model.LocalAccessFoundationModel" href="#alfred.fm.model.LocalAccessFoundationModel">LocalAccessFoundationModel</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="alfred.fm.model.FoundationModel.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, queries: Union[List[<a title="alfred.fm.query.query.Query" href="query/query.html#alfred.fm.query.query.Query">Query</a>], List[str], List[Tuple[str, str]]], batch_policy: str = 'dynamic', batch_size: int = 1024, score: bool = False, **kwargs) ‑> Union[List[<a title="alfred.fm.response.completion_response.CompletionResponse" href="response/completion_response.html#alfred.fm.response.completion_response.CompletionResponse">CompletionResponse</a>], List[<a title="alfred.fm.response.ranked_response.RankedResponse" href="response/ranked_response.html#alfred.fm.response.ranked_response.RankedResponse">RankedResponse</a>], List[OrderedDict[~KT, ~VT]]]</span>
</code></dt>
<dd>
<div class="desc"><p>This function is the main entry point for running queries through the foundation model.
It accepts raw query content and automatically converts it into query objects.
The function then determines whether to run the queries through the _generate_batch
or _score_batch method based on the type of queries. Finally, the function processes
the queries using one of two batching policies (dynamic, static) and passes them
through the foundation model.</p>
<p>:param queries: A list of queries
:type queries: Union[List[Query], List[str], List[Tuple[str, str]]]
:param batch_policy: The batching policy to use. Can be either 'dynamic' or 'static'
:type batch_policy: str
:param batch_size: The batch size to use for static batching or maximum batch size for dynamic batching
:type batch_size: int
:param score: Whether to run the queries through the _score_batch() method
:type score: bool
:param kwargs: Additional arguments to pass to the foundation model
:type kwargs: Any
:return: A list of responses
:rtype: Union[List[CompletionResponse], List[RankedResponse], List[OrderedDict]]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self,
            queries: Union[List[Query],
            List[str],
            List[Tuple[str, str]]],
            batch_policy: str = &#39;dynamic&#39;,
            batch_size: int = 1024,
            score: bool = False,
            **kwargs,
            ) -&gt; Union[List[CompletionResponse],
List[RankedResponse],
List[OrderedDict]]:
    &#34;&#34;&#34;
    This function is the main entry point for running queries through the foundation model.
    It accepts raw query content and automatically converts it into query objects.
    The function then determines whether to run the queries through the _generate_batch
    or _score_batch method based on the type of queries. Finally, the function processes
    the queries using one of two batching policies (dynamic, static) and passes them
    through the foundation model.

    :param queries: A list of queries
    :type queries: Union[List[Query], List[str], List[Tuple[str, str]]]
    :param batch_policy: The batching policy to use. Can be either &#39;dynamic&#39; or &#39;static&#39;
    :type batch_policy: str
    :param batch_size: The batch size to use for static batching or maximum batch size for dynamic batching
    :type batch_size: int
    :param score: Whether to run the queries through the _score_batch() method
    :type score: bool
    :param kwargs: Additional arguments to pass to the foundation model
    :type kwargs: Any
    :return: A list of responses
    :rtype: Union[List[CompletionResponse], List[RankedResponse], List[OrderedDict]]
    &#34;&#34;&#34;

    with_grad = kwargs.get(&#39;with_grad&#39;, False)
    no_tqdm = kwargs.get(&#39;no_tqdm&#39;, False)

    if type(queries[0]) in [RankedQuery, tuple]:
        score = True

    if batch_policy == &#39;static&#39;:
        # To near equally sized batches
        batched_queries = np.array_split(
            queries, len(queries) // batch_size
        )
    elif batch_policy == &#39;dynamic&#39;:
        DB = DynamicBatcher(queries, max_batch_size=batch_size)
        batched_queries = DB.batch()
    else:
        raise ValueError(f&#34;batch_policy {batch_policy} not supported&#34;)

    inferece_fn = self._score_batch if score else self._generate_batch
    logger.log(logging.INFO, f&#34;Inferring {len(batched_queries)} batches&#34;)

    with nullcontext() if with_grad else torch.no_grad():
        responses = []
        for batch_id, batch in enumerate(
                tqdm(batched_queries, disable=no_tqdm)):
            responses += inferece_fn(batch, **kwargs)

    if score:
        # Assuming candidates are the same for all queries for one
        # run/batch
        try:
            candidate_token_len = [
                len(x) for x in self.model.tokenizer(
                    list(
                        queries[0].candidates),
                    padding=True,
                    add_special_tokens=False).input_ids]
        except BaseException:
            logger.info(
                &#34;Unable to get candidate token length, defaulting to token length of 1&#34;)
            candidate_token_len = [1] * len(queries[0].candidates)
    else:
        candidate_token_len = None

    if batch_policy == &#39;dynamic&#39;:
        responses = DB.reorder(
            responses, candidate_token_len=candidate_token_len)

    return list(responses)</code></pre>
</details>
</dd>
<dt id="alfred.fm.model.FoundationModel.generate"><code class="name flex">
<span>def <span class="ident">generate</span></span>(<span>self, queries: Union[List[<a title="alfred.fm.query.completion_query.CompletionQuery" href="query/completion_query.html#alfred.fm.query.completion_query.CompletionQuery">CompletionQuery</a>], List[str]], batch_policy: str = 'dynamic', batch_size: int = 1024, **kwargs) ‑> List[<a title="alfred.fm.response.completion_response.CompletionResponse" href="response/completion_response.html#alfred.fm.response.completion_response.CompletionResponse">CompletionResponse</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>This function is a wrapper around the forward function for running
CompletionQuery objects through the foundation model. It returns a list
of CompletionResponse objects.</p>
<p>:param queries:
A list of CompletionQuery or raw query content (as string)
:type queries: Union[List[CompletionQuery], List[str]]
:param batch_policy: The batching policy to use. Can be either 'dynamic' or 'static'
:type batch_policy: str
:param batch_size: The batch size to use for static batching or maximum batch size for dynamic batching
:type batch_size: int
:param kwargs: Additional arguments to pass to the foundation model
:type kwargs: Any
:return: A list of CompletionResponse
:rtype: List[CompletionResponse]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate(self,
             queries: Union[List[CompletionQuery], List[str]],
             batch_policy: str = &#39;dynamic&#39;,
             batch_size: int = 1024,
             **kwargs,
             ) -&gt; List[CompletionResponse]:
    &#34;&#34;&#34;
    This function is a wrapper around the forward function for running
    CompletionQuery objects through the foundation model. It returns a list
    of CompletionResponse objects.

    :param queries:  A list of CompletionQuery or raw query content (as string)
    :type queries: Union[List[CompletionQuery], List[str]]
    :param batch_policy: The batching policy to use. Can be either &#39;dynamic&#39; or &#39;static&#39;
    :type batch_policy: str
    :param batch_size: The batch size to use for static batching or maximum batch size for dynamic batching
    :type batch_size: int
    :param kwargs: Additional arguments to pass to the foundation model
    :type kwargs: Any
    :return: A list of CompletionResponse
    :rtype: List[CompletionResponse]
    &#34;&#34;&#34;
    return self.forward(queries, batch_policy, batch_size, **kwargs)</code></pre>
</details>
</dd>
<dt id="alfred.fm.model.FoundationModel.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, queries: Union[<a title="alfred.fm.query.query.Query" href="query/query.html#alfred.fm.query.query.Query">Query</a>, str, Tuple[str, str], List[<a title="alfred.fm.query.query.Query" href="query/query.html#alfred.fm.query.query.Query">Query</a>], List[str]], **kwargs: Any) ‑> Union[str, <a title="alfred.fm.response.response.Response" href="response/response.html#alfred.fm.response.response.Response">Response</a>, List[<a title="alfred.fm.response.response.Response" href="response/response.html#alfred.fm.response.response.Response">Response</a>]]</span>
</code></dt>
<dd>
<div class="desc"><p>This function is the main entry point for users to run queries through the foundation model.
It accepts raw query content and automatically converts it into query objects.
The function then processes the queries and returns the responses in the appropriate format.
For single instance queries, a single response object is returned.</p>
<p>:param queries: A single query or a list of queries
:type queries: Union[Query, str, Tuple[str, str], List[Query], List[str]]
:param kwargs: Additional arguments to pass to the foundation model
:type kwargs: Any
:return: A single response or a list of responses
:rtype: Union[str, Response, List[Response]]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self,
        queries: Union[Query, str, Tuple[str, str], List[Query], List[str]],
        **kwargs: Any,
        ) -&gt; Union[str, Response, List[Response]]:
    &#34;&#34;&#34;
    This function is the main entry point for users to run queries through the foundation model.
    It accepts raw query content and automatically converts it into query objects.
    The function then processes the queries and returns the responses in the appropriate format.
    For single instance queries, a single response object is returned.

    :param queries: A single query or a list of queries
    :type queries: Union[Query, str, Tuple[str, str], List[Query], List[str]]
    :param kwargs: Additional arguments to pass to the foundation model
    :type kwargs: Any
    :return: A single response or a list of responses
    :rtype: Union[str, Response, List[Response]]
    &#34;&#34;&#34;
    if isinstance(queries, list):
        if type(queries[0]) in [tuple, RankedQuery]:
            score = True
        elif type(queries[0]) in [str, CompletionQuery]:
            score = False
        else:
            raise ValueError(f&#34;Unsupported query type {type(queries[0])}&#34;)
        return self.forward(queries, score=score, **kwargs)
    elif isinstance(queries, RankedQuery) or isinstance(queries, tuple):
        return self.forward([queries], score=True, **kwargs)[0]
    elif isinstance(queries, CompletionQuery) or isinstance(queries, str):
        return self._generate_batch(queries.load() if isinstance(
            queries, CompletionQuery) else [queries], **kwargs)[0]
    else:
        logger.warning(
            f&#34;Unsupported query type {type(queries)}, Attempting to run&#34;)
        return self.forward(queries, **kwargs)</code></pre>
</details>
</dd>
<dt id="alfred.fm.model.FoundationModel.score"><code class="name flex">
<span>def <span class="ident">score</span></span>(<span>self, queries: List[<a title="alfred.fm.query.ranked_query.RankedQuery" href="query/ranked_query.html#alfred.fm.query.ranked_query.RankedQuery">RankedQuery</a>], batch_policy: str = 'dynamic', batch_size: int = 1024, **kwargs: Any) ‑> List[<a title="alfred.fm.response.ranked_response.RankedResponse" href="response/ranked_response.html#alfred.fm.response.ranked_response.RankedResponse">RankedResponse</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>This function is a wrapper around the forward function
for running RankedQuery objects through the foundation model.
It returns a list of RankedResponse objects.</p>
<p>:param queries:
A list of RankedQuery
:type queries: List[RankedQuery]
:param batch_policy: The batching policy to use. Can be either 'dynamic' or 'static'
:type batch_policy: str
:param batch_size: The batch size to use for static batching or maximum batch size for dynamic batching
:type batch_size: int
:param kwargs: Additional arguments to pass to the foundation model
:type kwargs: Any
:return: A list of RankedResponse
:rtype: List[RankedResponse]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score(self,
          queries: List[RankedQuery],
          batch_policy: str = &#39;dynamic&#39;,
          batch_size: int = 1024,
          **kwargs: Any,
          ) -&gt; List[RankedResponse]:
    &#34;&#34;&#34;
    This function is a wrapper around the forward function
    for running RankedQuery objects through the foundation model.
    It returns a list of RankedResponse objects.

    :param queries:  A list of RankedQuery
    :type queries: List[RankedQuery]
    :param batch_policy: The batching policy to use. Can be either &#39;dynamic&#39; or &#39;static&#39;
    :type batch_policy: str
    :param batch_size: The batch size to use for static batching or maximum batch size for dynamic batching
    :type batch_size: int
    :param kwargs: Additional arguments to pass to the foundation model
    :type kwargs: Any
    :return: A list of RankedResponse
    :rtype: List[RankedResponse]
    &#34;&#34;&#34;

    return self.forward(
        queries,
        batch_policy,
        batch_size,
        score=True,
        **kwargs)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="alfred.fm.model.LocalAccessFoundationModel"><code class="flex name class">
<span>class <span class="ident">LocalAccessFoundationModel</span></span>
<span>(</span><span>model_string: str, local_path: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Generic interface for foundation model class</p>
<p>Initializes the LocalAccessFoundationModel class,
which wraps a local serving model such as PyTorch or HuggingFace.</p>
<p>:param model_string: The model string that defines the model type
(e.g. gpt2 for HuggingFace GPT-2)
:type model_string: str
:param local_path: (optional) The path to the local model directories.
If not provided, the default path ~/.model_cache/{model_string} will be used.
:type local_path: str</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LocalAccessFoundationModel(FoundationModel):
    def __init__(self,
                 model_string: str,
                 local_path: Optional[str] = None):
        &#34;&#34;&#34;
        Initializes the LocalAccessFoundationModel class,
         which wraps a local serving model such as PyTorch or HuggingFace.

        :param model_string: The model string that defines the model type
                                (e.g. gpt2 for HuggingFace GPT-2)
        :type model_string: str
        :param local_path: (optional) The path to the local model directories.
               If not provided, the default path ~/.model_cache/{model_string} will be used.
        :type local_path: str
        &#34;&#34;&#34;
        self.local_path = local_path
        if local_path is None:
            self.local_path = os.path.join(
                os.path.expanduser(&#39;~&#39;), &#39;.model_cache&#39;, model_string)
        self.model_string = model_string</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="alfred.fm.model.FoundationModel" href="#alfred.fm.model.FoundationModel">FoundationModel</a></li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="alfred.fm.dummy.DummyModel" href="dummy.html#alfred.fm.dummy.DummyModel">DummyModel</a></li>
<li><a title="alfred.fm.huggingface.HuggingFaceModel" href="huggingface.html#alfred.fm.huggingface.HuggingFaceModel">HuggingFaceModel</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="alfred.fm.model.FoundationModel" href="#alfred.fm.model.FoundationModel">FoundationModel</a></b></code>:
<ul class="hlist">
<li><code><a title="alfred.fm.model.FoundationModel.forward" href="#alfred.fm.model.FoundationModel.forward">forward</a></code></li>
<li><code><a title="alfred.fm.model.FoundationModel.generate" href="#alfred.fm.model.FoundationModel.generate">generate</a></code></li>
<li><code><a title="alfred.fm.model.FoundationModel.run" href="#alfred.fm.model.FoundationModel.run">run</a></code></li>
<li><code><a title="alfred.fm.model.FoundationModel.score" href="#alfred.fm.model.FoundationModel.score">score</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="alfred.fm" href="index.html">alfred.fm</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="alfred.fm.model.APIAccessFoundationModel" href="#alfred.fm.model.APIAccessFoundationModel">APIAccessFoundationModel</a></code></h4>
</li>
<li>
<h4><code><a title="alfred.fm.model.FoundationModel" href="#alfred.fm.model.FoundationModel">FoundationModel</a></code></h4>
<ul class="">
<li><code><a title="alfred.fm.model.FoundationModel.forward" href="#alfred.fm.model.FoundationModel.forward">forward</a></code></li>
<li><code><a title="alfred.fm.model.FoundationModel.generate" href="#alfred.fm.model.FoundationModel.generate">generate</a></code></li>
<li><code><a title="alfred.fm.model.FoundationModel.run" href="#alfred.fm.model.FoundationModel.run">run</a></code></li>
<li><code><a title="alfred.fm.model.FoundationModel.score" href="#alfred.fm.model.FoundationModel.score">score</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="alfred.fm.model.LocalAccessFoundationModel" href="#alfred.fm.model.LocalAccessFoundationModel">LocalAccessFoundationModel</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>