{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Getting Started\n",
    "\n",
    "\n",
    "Welcome to Alfred! Alfred is a convinient toolbox to take advantage of LLMs for data annotations. Here let's introduce some basic concepts and components for you to interact with it.\n",
    "\n",
    "\n",
    "\n",
    "## Client\n",
    "The base class for user interaction is a `Client`\n",
    "To use one simply define with `alfred.Client`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from alfred.client import Client\n",
    "\n",
    "t5_small = Client(model_type='huggingface', model='t5-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run for completions, simply call `client(<your_text_prompt>)` or `client.run(<your_text_prompt>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_small(\"Translate English to German: To be, or not to be, that is the question.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also define where to look for or download your huggingface models. You can simply set it to be your default huggingface cache directory. To set the directory simple add a `local_path` argument to the `client` constructor:\n",
    "`Client(..., local_path=<path_to_model>)`\n",
    "\n",
    "#### Client for Remote Models\n",
    "\n",
    "Want to run your LLMs on the cloud and connect alfred remotely to it? Here are the steps to do it:\n",
    "\n",
    "1. Setup a remote server by running\n",
    "\n",
    "```python alfred.run_server --model_type <model_type> --model <model_name> --local_path <model_ckpt_dir> --port <port_number>```\n",
    "\n",
    "2. Run the Alfred Client locally! That simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0pp = Client(end_point=\"\", ssh_tunnel=True, ssh_node=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0pp(\"Who wrote \\\"To be, or not to be, that is the question.\\\" ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Typed Queries and Responses\n",
    "\n",
    "\n",
    "\n",
    "A `Query` is the primary input object that the `Client` interacts with. There are currently two types of queries: \n",
    "\n",
    "   - `CompletionQuery`\n",
    "   - `RankedQuery`\n",
    "\n",
    "`CompletionQuery` is used for completion or generation tasks, while the `RankedQuery` is used for ranking candidates or scoring tasks. The `Client` will automatically process the input query and return the appropriate `CompletionResponse` or `RankedResponse`, corresponding to the input query type.\n",
    "\n",
    "Previously, the `Client` accepted a string input directly. However, internally the `Client` treated it as a `CompletionQuery`. As such, running `Client(<your_text_prompt>)` is equivalent to running `Client(CompletionQuery(<your_text_prompt>))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alfred.fm.query import CompletionQuery, RankedQuery\n",
    "\n",
    "t0pp(CompletionQuery(\"Who wrote \\\"To be, or not to be, that is the question.\\\" ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = t0pp(CompletionQuery(\"Who wrote \\\"To be, or not to be, that is the question.\\\" ?\"))\n",
    "\n",
    "print(response, type(response))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = t0pp(RankedQuery(\"Who wrote \\\"To be, or not to be, that is the question.\\\" ?\", candidates=['Shakespeare', 'Webster']))\n",
    "\n",
    "print(response, type(response))\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
